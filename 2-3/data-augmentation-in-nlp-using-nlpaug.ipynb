{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:46:18.932166Z",
     "iopub.status.busy": "2021-07-25T18:46:18.931672Z",
     "iopub.status.idle": "2021-07-25T18:46:28.545285Z",
     "shell.execute_reply": "2021-07-25T18:46:28.543933Z",
     "shell.execute_reply.started": "2021-07-25T18:46:18.932062Z"
    },
    "id": "n8-G42lKMmGA",
    "outputId": "3918e740-4333-434a-8335-b43033a0adbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlpaug in /home/daniel/anaconda3/lib/python3.7/site-packages (1.1.9)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /home/daniel/anaconda3/lib/python3.7/site-packages (from nlpaug) (1.21.6)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /home/daniel/anaconda3/lib/python3.7/site-packages (from nlpaug) (1.2.4)\n",
      "Requirement already satisfied: requests>=2.22.0 in /home/daniel/anaconda3/lib/python3.7/site-packages (from nlpaug) (2.28.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/daniel/anaconda3/lib/python3.7/site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/daniel/anaconda3/lib/python3.7/site-packages (from pandas>=1.2.0->nlpaug) (2021.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/daniel/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->nlpaug) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/daniel/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->nlpaug) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/daniel/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->nlpaug) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/daniel/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->nlpaug) (2021.5.30)\n",
      "Requirement already satisfied: six>=1.5 in /home/daniel/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->nlpaug) (1.15.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nlpaug是一个由Edward Ma开发的开源Python库，该库提供了一系列字符、单词和句子的文本增强器，一般情况下只需3-5行代码即可应用。 如果要是用同义词或反义词扩展，则需要安装NLTK库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJdIts_4M5Me"
   },
   "source": [
    "# Downloading required data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Pp2KF30OVU2"
   },
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:46:28.547571Z",
     "iopub.status.busy": "2021-07-25T18:46:28.547277Z",
     "iopub.status.idle": "2021-07-25T18:46:55.531816Z",
     "shell.execute_reply": "2021-07-25T18:46:55.530822Z",
     "shell.execute_reply.started": "2021-07-25T18:46:28.547541Z"
    },
    "id": "tHUSSHH1KL1w",
    "outputId": "bcd0b073-3dfd-4935-f05c-6cf2aa7b572c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/daniel/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /home/daniel/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:46:55.534167Z",
     "iopub.status.busy": "2021-07-25T18:46:55.533840Z",
     "iopub.status.idle": "2021-07-25T18:46:59.686512Z",
     "shell.execute_reply": "2021-07-25T18:46:59.685245Z",
     "shell.execute_reply.started": "2021-07-25T18:46:55.534137Z"
    },
    "id": "DEud_6KHKL2E"
   },
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as naf\n",
    "\n",
    "from nlpaug.util import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: This is a test.\n",
      "Augmented: 8Thiys is a @te5st.\n"
     ]
    }
   ],
   "source": [
    "# Character Augmentation\n",
    "# Character augmentation modifies characters in a text, such as swapping characters or inserting characters.\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "# Initialize a character augmenter\n",
    "aug = nac.RandomCharAug(action=\"insert\")\n",
    "\n",
    "# Example text\n",
    "text = \"This is a test.\"\n",
    "\n",
    "# Augment the text\n",
    "augmented_text = aug.augment(text)\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Augmented:\", augmented_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: This is a test.\n",
      "Augmented: This exist a trial run.\n"
     ]
    }
   ],
   "source": [
    "# Word Augmentation\n",
    "# Word augmentation modifies words in a text, such as substituting words with synonyms or random words.\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Initialize a word augmenter\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# Example text\n",
    "text = \"This is a test.\"\n",
    "\n",
    "# Augment the text\n",
    "augmented_text = aug.augment(text)\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Augmented:\", augmented_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03328132629394531,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading model.safetensors",
       "rate": null,
       "total": 548105171,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3808582692764bcd85141ec12ebc9d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029076576232910156,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading generation_config.json",
       "rate": null,
       "total": 124,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618858c33c4848ac8cf6c4ec9e1ef1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: This is a test.\n",
      "Augmented: This is a test. I had no problem with any of these issues. The problem was with the timing. When I turn on the power, I lose about 15 milliseconds on a normal setting. The power was working great so I turned off my power to turn it back on. The problem was a bit of a mystery to me. So I put it down to the fact that I didn't think about it. I wasn't sure how long it would take the problem to get solved, or what was causing it. I'm not sure how many times I did that. I guess it's just a coincidence.\n",
      "\n",
      "\n",
      "My first thought was that maybe the power was running out. It was a couple of seconds, and then the power was out.\n",
      "\n",
      "This is an update from July 13th. I didn't think that was too bad. After a few more days, I was able to remove all the power from my laptop. However, I noticed that the power was still running out. I did not notice any problems, even though I was trying to use a standard Macbook Pro for the first time. It's probably the only way to remove power that will cause the problems. There are a few things to note. First off, the power is working fine with only one mouse. No problems with the USB port. No problems with the power cable. I think the only other thing I can think of is that the power is working fine for the moment.\n",
      "\n",
      "\n",
      "Finally, the mouse was a bit more sensitive. I was able to adjust the brightness of the screen, without any issues. After the first few days, my mouse would never wake up when I turn it off. Then, with a couple of days of hard sleep, I found the mouse would wake up and work fine. I tried this after about 4 days and couldn't find any reason to change the brightness to 8:20. It never seemed to work for me. The only thing I can think of is that this is a test, I have never had any issues. The problem was with the timing. When I turn on the power, I lose about 15 milliseconds on a normal setting. The power was working great so I turned off my power to turn it back on. The problem was a bit of a mystery to me. So I put it down to the fact that I didn't think about it. I wasn't sure how long it would take the problem to get solved, or what was\n"
     ]
    }
   ],
   "source": [
    "# Sentence Augmentation\n",
    "# Sentence augmentation can insert new sentences into the text. However, sentence-level augmentation is less common and may require a specific context to be useful.\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "\n",
    "# Initialize a sentence augmenter\n",
    "aug = nas.ContextualWordEmbsForSentenceAug()\n",
    "\n",
    "# Example text\n",
    "text = \"This is a test.\"\n",
    "\n",
    "# Augment the text\n",
    "augmented_text = aug.augment(text)\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Augmented:\", augmented_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: This is a test.\n",
      "Augmented: vThirs be a tFes% thyroxine.\n"
     ]
    }
   ],
   "source": [
    "# Flow Augmentation\n",
    "# Flow augmentation allows you to apply a sequence of augmentations.\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.flow as naf\n",
    "\n",
    "# Initialize individual augmenters\n",
    "char_aug = nac.RandomCharAug(action=\"insert\")\n",
    "word_aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# Combine augmenters into a flow\n",
    "flow = naf.Sequential([char_aug, word_aug])\n",
    "\n",
    "# Example text\n",
    "text = \"This is a test.\"\n",
    "\n",
    "# Augment the text\n",
    "augmented_text = flow.augment(text)\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Augmented:\", augmented_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: This is a test.\n",
      "Augmented: ThLiNs is a taes8t.\n"
     ]
    }
   ],
   "source": [
    "# Using the Action Utility\n",
    "# Action is an enum that defines possible actions for augmenters, such as insert, substitute, etc. It's used to specify what kind of augmentation to perform.\n",
    "\n",
    "# Here's an example of using Action with a character augmenter:\n",
    "from nlpaug.util import Action\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "# Initialize a character augmenter with the 'insert' action\n",
    "aug = nac.RandomCharAug(action=Action.INSERT)\n",
    "\n",
    "# Example text\n",
    "text = \"This is a test.\"\n",
    "\n",
    "# Augment the text\n",
    "augmented_text = aug.augment(text)\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Augmented:\", augmented_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZxVL2NVKL2W"
   },
   "source": [
    "## Loading the data and getting basic idea "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:46:59.689575Z",
     "iopub.status.busy": "2021-07-25T18:46:59.689091Z",
     "iopub.status.idle": "2021-07-25T18:46:59.778868Z",
     "shell.execute_reply": "2021-07-25T18:46:59.778131Z",
     "shell.execute_reply.started": "2021-07-25T18:46:59.689524Z"
    },
    "id": "fxx4dutMKL2X",
    "outputId": "e39698da-02a0-4d29-b6be-40cf9e704d0e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet= pd.read_csv('nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('nlp-getting-started/test.csv')\n",
    "tweet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:46:59.780395Z",
     "iopub.status.busy": "2021-07-25T18:46:59.780113Z",
     "iopub.status.idle": "2021-07-25T18:46:59.786456Z",
     "shell.execute_reply": "2021-07-25T18:46:59.785645Z",
     "shell.execute_reply.started": "2021-07-25T18:46:59.780366Z"
    },
    "id": "dEAGD_0uKL2d",
    "outputId": "b84ad422-6725-4e9a-c8e5-e28a97f965e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7613 rows and 5 columns in train\n",
      "There are 3263 rows and 4 columns in train\n"
     ]
    }
   ],
   "source": [
    "print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\n",
    "print('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:46:59.788129Z",
     "iopub.status.busy": "2021-07-25T18:46:59.787662Z",
     "iopub.status.idle": "2021-07-25T18:46:59.805166Z",
     "shell.execute_reply": "2021-07-25T18:46:59.803794Z",
     "shell.execute_reply.started": "2021-07-25T18:46:59.788081Z"
    },
    "id": "Yls-W2oJKL2h"
   },
   "outputs": [],
   "source": [
    "tweet=tweet.drop(['keyword','location'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_NmXrD1KL2m"
   },
   "source": [
    "## Class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFYvDIyfKL2n"
   },
   "source": [
    "Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:46:59.806898Z",
     "iopub.status.busy": "2021-07-25T18:46:59.806579Z",
     "iopub.status.idle": "2021-07-25T18:46:59.985702Z",
     "shell.execute_reply": "2021-07-25T18:46:59.984588Z",
     "shell.execute_reply.started": "2021-07-25T18:46:59.806869Z"
    },
    "id": "JJuD3jmEKL2o",
    "outputId": "98c5b04a-234e-40c1-93cf-fb02932c0738"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(29.597222222222214, 0.5, 'samples')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tweet.target.value_counts()\n",
    "sns.barplot(x.index,x)\n",
    "plt.gca().set_ylabel('samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOdBX3AJKL2t"
   },
   "source": [
    "ohh,as expected ! There is a class distribution.There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27pnGpPgKL2u"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:46:59.989559Z",
     "iopub.status.busy": "2021-07-25T18:46:59.988997Z",
     "iopub.status.idle": "2021-07-25T18:47:26.041054Z",
     "shell.execute_reply": "2021-07-25T18:47:26.040228Z",
     "shell.execute_reply.started": "2021-07-25T18:46:59.989521Z"
    },
    "id": "uAXWJIqjKL2v"
   },
   "outputs": [],
   "source": [
    "# model_type: word2vec, glove or fasttext\n",
    "aug_w2v = naw.WordEmbsAug(\n",
    "#     model_type='word2vec', model_path='../input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin',\n",
    "    model_type='glove', model_path='glove-global-vectors-for-word-representation/glove.6B.100d.txt',\n",
    "    action=\"substitute\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:47:26.043623Z",
     "iopub.status.busy": "2021-07-25T18:47:26.043036Z",
     "iopub.status.idle": "2021-07-25T18:47:26.051967Z",
     "shell.execute_reply": "2021-07-25T18:47:26.050850Z",
     "shell.execute_reply.started": "2021-07-25T18:47:26.043582Z"
    },
    "id": "-AiLFrswKL20",
    "outputId": "8689ec99-ebe5-4440-cb69-8a16e18a3038"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tweet.iloc[0]['text']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:47:26.053932Z",
     "iopub.status.busy": "2021-07-25T18:47:26.053512Z",
     "iopub.status.idle": "2021-07-25T18:47:26.376657Z",
     "shell.execute_reply": "2021-07-25T18:47:26.375643Z",
     "shell.execute_reply.started": "2021-07-25T18:47:26.053873Z"
    },
    "id": "b_1U_PuvKL26",
    "outputId": "ea9203cc-ed41-48a1-c764-3a58c1fb3996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Text:\n",
      "Our Deeds are the Reason by day # katrina May ALLAH Forgive us all\n",
      "Our Deeds are the Reason present can # hurricane May ALLAH Forgive us all\n",
      "Our Deeds them the Reason of can # earthquake May ALLAH Forgive us them\n",
      "Our Deeds ones the Reason of this # jolted May ALLAH Forgive us three\n",
      "Our Deeds should the Reason of one # earthquake May ALLAH Forgive us should\n"
     ]
    }
   ],
   "source": [
    "aug_w2v.aug_p=0.2\n",
    "print(\"Augmented Text:\")\n",
    "for ii in range(5):\n",
    "    augmented_text = aug_w2v.augment(text)\n",
    "    print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:47:26.378949Z",
     "iopub.status.busy": "2021-07-25T18:47:26.378262Z",
     "iopub.status.idle": "2021-07-25T18:47:26.392526Z",
     "shell.execute_reply": "2021-07-25T18:47:26.391259Z",
     "shell.execute_reply.started": "2021-07-25T18:47:26.378906Z"
    },
    "id": "p1JOQpbEKL29",
    "outputId": "f2111a27-4f77-4826-c584-922c52c4b3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train (6471, 3)\n",
      "Shape of Validation  (1142, 3)\n"
     ]
    }
   ],
   "source": [
    "train,valid=train_test_split(tweet,test_size=0.15)\n",
    "print('Shape of train',train.shape)\n",
    "print(\"Shape of Validation \",valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:47:26.395271Z",
     "iopub.status.busy": "2021-07-25T18:47:26.394418Z",
     "iopub.status.idle": "2021-07-25T18:47:26.406663Z",
     "shell.execute_reply": "2021-07-25T18:47:26.405141Z",
     "shell.execute_reply.started": "2021-07-25T18:47:26.395229Z"
    },
    "id": "gNv3AhTLKL3C"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def augment_text(df,samples=300,pr=0.2):\n",
    "    aug_w2v.aug_p=pr\n",
    "    new_text=[]\n",
    "    \n",
    "    ##dropping samples from validation\n",
    "    df_n=df[df.target==1].reset_index(drop=True)\n",
    "\n",
    "    ## data augmentation loop\n",
    "    for i in tqdm(np.random.randint(0,len(df_n),samples)):\n",
    "        \n",
    "            text = df_n.iloc[i]['text']\n",
    "            augmented_text = aug_w2v.augment(text)\n",
    "            new_text.append(augmented_text)\n",
    "    \n",
    "    \n",
    "    ## dataframe\n",
    "    new=pd.DataFrame({'text':new_text,'target':1})\n",
    "    df=shuffle(df.append(new).reset_index(drop=True))\n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:47:26.409323Z",
     "iopub.status.busy": "2021-07-25T18:47:26.408563Z",
     "iopub.status.idle": "2021-07-25T18:48:27.174648Z",
     "shell.execute_reply": "2021-07-25T18:48:27.173507Z",
     "shell.execute_reply.started": "2021-07-25T18:47:26.409277Z"
    },
    "id": "ZMYRx5rdKL3K",
    "outputId": "3e883c44-14f5-4a97-9b30-bbb2a6deaedb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:09<00:00,  5.74it/s]\n"
     ]
    }
   ],
   "source": [
    "train = augment_text(train,samples=400)   ## change samples to 0 for no augmentation\n",
    "tweet = train.append(valid).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:27.177184Z",
     "iopub.status.busy": "2021-07-25T18:48:27.176434Z",
     "iopub.status.idle": "2021-07-25T18:48:27.188711Z",
     "shell.execute_reply": "2021-07-25T18:48:27.187506Z",
     "shell.execute_reply.started": "2021-07-25T18:48:27.177132Z"
    },
    "id": "itPb5__NKL3O"
   },
   "outputs": [],
   "source": [
    "df=pd.concat([tweet,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:27.191431Z",
     "iopub.status.busy": "2021-07-25T18:48:27.190652Z",
     "iopub.status.idle": "2021-07-25T18:48:27.200395Z",
     "shell.execute_reply": "2021-07-25T18:48:27.199325Z",
     "shell.execute_reply.started": "2021-07-25T18:48:27.191377Z"
    },
    "id": "p2t3hzMtKL3S",
    "outputId": "eae0a243-c010-4d74-a439-f7280f7282fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11276, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPN4iEyxKL4L"
   },
   "source": [
    "## GloVe for Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vh4FKTZKL4M"
   },
   "source": [
    "Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here.\n",
    "This pipeline is described in [this](https://neptune.ai/blog/document-classification-small-datasets) article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:27.202970Z",
     "iopub.status.busy": "2021-07-25T18:48:27.202269Z",
     "iopub.status.idle": "2021-07-25T18:48:27.214900Z",
     "shell.execute_reply": "2021-07-25T18:48:27.213342Z",
     "shell.execute_reply.started": "2021-07-25T18:48:27.202917Z"
    },
    "id": "kgo6HEgfKL4N"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:27.220498Z",
     "iopub.status.busy": "2021-07-25T18:48:27.220009Z",
     "iopub.status.idle": "2021-07-25T18:48:31.087839Z",
     "shell.execute_reply": "2021-07-25T18:48:31.086926Z",
     "shell.execute_reply.started": "2021-07-25T18:48:27.220451Z"
    },
    "id": "Xby3aXQ-KL4Q",
    "outputId": "2f703972-c8f7-4500-ce76-bb6c0026252f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 6626/11276 [00:01<00:01, 4086.87it/s]"
     ]
    }
   ],
   "source": [
    "corpus=create_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:31.089471Z",
     "iopub.status.busy": "2021-07-25T18:48:31.089178Z",
     "iopub.status.idle": "2021-07-25T18:48:49.806417Z",
     "shell.execute_reply": "2021-07-25T18:48:49.805496Z",
     "shell.execute_reply.started": "2021-07-25T18:48:31.089441Z"
    },
    "id": "m3HmcsHCKL4f"
   },
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:49.808299Z",
     "iopub.status.busy": "2021-07-25T18:48:49.807782Z",
     "iopub.status.idle": "2021-07-25T18:48:50.109997Z",
     "shell.execute_reply": "2021-07-25T18:48:50.109169Z",
     "shell.execute_reply.started": "2021-07-25T18:48:49.808262Z"
    },
    "id": "IdMQiskdKL4h"
   },
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:50.111448Z",
     "iopub.status.busy": "2021-07-25T18:48:50.111002Z",
     "iopub.status.idle": "2021-07-25T18:48:50.117761Z",
     "shell.execute_reply": "2021-07-25T18:48:50.116427Z",
     "shell.execute_reply.started": "2021-07-25T18:48:50.111417Z"
    },
    "id": "6AwnhzsaKL4k",
    "outputId": "85d8aac0-93a0-46bd-c7f4-8361c04fc40d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 19040\n"
     ]
    }
   ],
   "source": [
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:50.119345Z",
     "iopub.status.busy": "2021-07-25T18:48:50.119062Z",
     "iopub.status.idle": "2021-07-25T18:48:50.206872Z",
     "shell.execute_reply": "2021-07-25T18:48:50.205517Z",
     "shell.execute_reply.started": "2021-07-25T18:48:50.119320Z"
    },
    "id": "0Kw8h7pqKL4m",
    "outputId": "99ed14ff-c440-461a-b551-2963b18f9cb0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19040/19040 [00:00<00:00, 259944.24it/s]\n"
     ]
    }
   ],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuJrV3iwKL4p"
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:50.209078Z",
     "iopub.status.busy": "2021-07-25T18:48:50.208616Z",
     "iopub.status.idle": "2021-07-25T18:48:50.569622Z",
     "shell.execute_reply": "2021-07-25T18:48:50.568506Z",
     "shell.execute_reply.started": "2021-07-25T18:48:50.209005Z"
    },
    "id": "P8JW2B4mKL4p"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=1e-5)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:50.573971Z",
     "iopub.status.busy": "2021-07-25T18:48:50.573648Z",
     "iopub.status.idle": "2021-07-25T18:48:50.583922Z",
     "shell.execute_reply": "2021-07-25T18:48:50.582747Z",
     "shell.execute_reply.started": "2021-07-25T18:48:50.573943Z"
    },
    "id": "o7gOFunSKL4r",
    "outputId": "b87cfefa-378c-4a4a-b860-686510d2d155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 100)           1904100   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,984,601\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 1,904,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:50.585944Z",
     "iopub.status.busy": "2021-07-25T18:48:50.585618Z",
     "iopub.status.idle": "2021-07-25T18:48:50.593432Z",
     "shell.execute_reply": "2021-07-25T18:48:50.592211Z",
     "shell.execute_reply.started": "2021-07-25T18:48:50.585915Z"
    },
    "id": "kaQHdfPwKL4v"
   },
   "outputs": [],
   "source": [
    "train_df=tweet_pad[:tweet.shape[0]]\n",
    "test_df=tweet_pad[tweet.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x18oDpcxPAnw"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:50.596502Z",
     "iopub.status.busy": "2021-07-25T18:48:50.596115Z",
     "iopub.status.idle": "2021-07-25T18:48:50.606222Z",
     "shell.execute_reply": "2021-07-25T18:48:50.605111Z",
     "shell.execute_reply.started": "2021-07-25T18:48:50.596469Z"
    },
    "id": "vBS70maMKL4x"
   },
   "outputs": [],
   "source": [
    "X_train,y_train = train_df[:train.shape[0]],tweet['target'][:train.shape[0]]\n",
    "\n",
    "X_test,y_test= train_df[train.shape[0]:],tweet['target'][train.shape[0]:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T18:48:50.608376Z",
     "iopub.status.busy": "2021-07-25T18:48:50.607880Z",
     "iopub.status.idle": "2021-07-25T19:07:00.005492Z",
     "shell.execute_reply": "2021-07-25T19:07:00.004033Z",
     "shell.execute_reply.started": "2021-07-25T18:48:50.608345Z"
    },
    "id": "rkI23SqJKL4y",
    "outputId": "b07e1e54-31c6-4a1a-a505-cbce5525420a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1718/1718 - 112s - loss: 0.6682 - accuracy: 0.5832 - val_loss: 0.5375 - val_accuracy: 0.7557\n",
      "Epoch 2/10\n",
      "1718/1718 - 108s - loss: 0.5310 - accuracy: 0.7473 - val_loss: 0.5167 - val_accuracy: 0.7671\n",
      "Epoch 3/10\n",
      "1718/1718 - 110s - loss: 0.5127 - accuracy: 0.7583 - val_loss: 0.4975 - val_accuracy: 0.7785\n",
      "Epoch 4/10\n",
      "1718/1718 - 108s - loss: 0.5017 - accuracy: 0.7647 - val_loss: 0.4946 - val_accuracy: 0.7828\n",
      "Epoch 5/10\n",
      "1718/1718 - 108s - loss: 0.4947 - accuracy: 0.7695 - val_loss: 0.4891 - val_accuracy: 0.7933\n",
      "Epoch 6/10\n",
      "1718/1718 - 108s - loss: 0.4917 - accuracy: 0.7741 - val_loss: 0.4824 - val_accuracy: 0.7968\n",
      "Epoch 7/10\n",
      "1718/1718 - 109s - loss: 0.4916 - accuracy: 0.7734 - val_loss: 0.4749 - val_accuracy: 0.7986\n",
      "Epoch 8/10\n",
      "1718/1718 - 108s - loss: 0.4881 - accuracy: 0.7735 - val_loss: 0.4689 - val_accuracy: 0.7960\n",
      "Epoch 9/10\n",
      "1718/1718 - 110s - loss: 0.4869 - accuracy: 0.7776 - val_loss: 0.4728 - val_accuracy: 0.8039\n",
      "Epoch 10/10\n",
      "1718/1718 - 108s - loss: 0.4859 - accuracy: 0.7794 - val_loss: 0.4721 - val_accuracy: 0.7986\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=4,epochs=10,validation_data=(X_test,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwJpG5CvKL43"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T19:07:00.009153Z",
     "iopub.status.busy": "2021-07-25T19:07:00.008637Z",
     "iopub.status.idle": "2021-07-25T19:07:00.806395Z",
     "shell.execute_reply": "2021-07-25T19:07:00.805166Z",
     "shell.execute_reply.started": "2021-07-25T19:07:00.009107Z"
    },
    "id": "Im8hPfElKL43"
   },
   "outputs": [],
   "source": [
    "y_pre=model.predict(X_test)\n",
    "y_pre=np.round(y_pre).astype(int).reshape(1142)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T19:07:00.808107Z",
     "iopub.status.busy": "2021-07-25T19:07:00.807787Z",
     "iopub.status.idle": "2021-07-25T19:07:00.819650Z",
     "shell.execute_reply": "2021-07-25T19:07:00.818472Z",
     "shell.execute_reply.started": "2021-07-25T19:07:00.808076Z"
    },
    "id": "azIU0qYcKL45",
    "outputId": "f4fc8f08-13dc-4b4d-a7b0-9293596efc34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7912547807475032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_pre,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***IF YOU LOVE MY NOTEBOOK PLEASE FREE TO UPVOTE***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
